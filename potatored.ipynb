{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebadc2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will import the necessary Library\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime as dt\n",
    "\n",
    "# For Evalution we will use these library\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For model building we will use these library\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "# For PLotting we will use these library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "%matplotlib inline\n",
    "from plotly.offline import init_notebook_mode,iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd6743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.4577e-04\n",
      "Epoch 4: val_loss improved from 0.00110 to 0.00102, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 8.4626e-04 - val_loss: 0.0010 - learning_rate: 0.0010\n",
      "Epoch 5/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.9399e-04\n",
      "Epoch 5: val_loss did not improve from 0.00102\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 8.9425e-04 - val_loss: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 6/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.9020e-04\n",
      "Epoch 6: val_loss improved from 0.00102 to 0.00085, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 8.8938e-04 - val_loss: 8.5167e-04 - learning_rate: 0.0010\n",
      "Epoch 7/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.4430e-04\n",
      "Epoch 7: val_loss did not improve from 0.00085\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 7.4459e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 8/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.9816e-04\n",
      "Epoch 8: val_loss improved from 0.00085 to 0.00083, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 8.9650e-04 - val_loss: 8.3238e-04 - learning_rate: 0.0010\n",
      "Epoch 9/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.3165e-04\n",
      "Epoch 9: val_loss improved from 0.00083 to 0.00073, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 9.2704e-04 - val_loss: 7.3277e-04 - learning_rate: 0.0010\n",
      "Epoch 10/220\n",
      "\u001b[1m75/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.3885e-04\n",
      "Epoch 10: val_loss improved from 0.00073 to 0.00069, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 6.3838e-04 - val_loss: 6.8530e-04 - learning_rate: 0.0010\n",
      "Epoch 11/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.7827e-04\n",
      "Epoch 11: val_loss did not improve from 0.00069\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 6.7743e-04 - val_loss: 6.9116e-04 - learning_rate: 0.0010\n",
      "Epoch 12/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.5898e-04\n",
      "Epoch 12: val_loss improved from 0.00069 to 0.00065, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 5.6017e-04 - val_loss: 6.5191e-04 - learning_rate: 0.0010\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Train data RMSE:  2.66874293799759\n",
      "Train data MSE:  7.12218886911201\n",
      "Train data MAE:  1.5787953195245692\n",
      "-------------------------------------------------------------------------------------\n",
      "Test data RMSE:  2.8985386678202554\n",
      "Test data MSE:  8.401526408849222\n",
      "Test data MAE:  1.815101181277792\n",
      "Train data explained variance regression score: 0.971202671254778\n",
      "Test data explained variance regression score: 0.9632209915834695\n",
      "Train data R2 score: 0.9711973313222315\n",
      "Test data R2 score: 0.9629664204239706\n",
      "Train data MGD:  0.004259167475389127\n",
      "Test data MGD:  0.003441287140166806\n",
      "----------------------------------------------------------------------\n",
      "Train data MPD:  0.16007993426674572\n",
      "Test data MPD:  0.16108955887168785\n",
      "Train predicted data:  (3595, 1)\n",
      "Test predicted data:  (3595, 1)\n",
      "Output of predicted next days:  30\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "[31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59 60]\n",
      "[31.0, 28.999999999999996, 30.0, 30.0, 28.999999999999996, 28.999999999999996, 28.999999999999996, 29.25, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 31.0, 31.0, 32.5, 32.0, 32.0, 34.0, 34.0, 33.8, 33.33, 34.0, 33.6, 33.4, 33.5, 33.0, 33.5]\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('E:/cryptics/dataset/Potato_Red.csv')\n",
    "df\n",
    "closedf = df[['sn','price']]\n",
    "print(\"Shape of close dataframe:\", closedf.shape)\n",
    "del closedf['sn']\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "closedf=scaler.fit_transform(np.array(closedf).reshape(-1,1))\n",
    "print(closedf.shape)\n",
    "\n",
    "training_size=int(len(closedf)*0.70)\n",
    "test_size=len(closedf)-training_size\n",
    "train_data,test_data=closedf[0:training_size,:],closedf[training_size:len(closedf),:1]\n",
    "\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "time_step = 30\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "\n",
    "#red potato\n",
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=(None,1),activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "\n",
    "#early stopping and model checking\n",
    "\n",
    "#Early Stopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', #value being monitored for improvement\n",
    "                          min_delta = 0.001,         #Abs value and is the main change required before we stop\n",
    "                          mode='auto',\n",
    "                          patience = 6,             #no of epocs we wait before stopping\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keep the best weigts once stopped\n",
    "#Model Checkpoint\n",
    "checkpoint = ModelCheckpoint(monitor = \"val_loss\",\n",
    "                             mode ='auto',\n",
    "                             filepath = \"E:/cryptics/dataset/best_model.keras\",\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             )\n",
    "\n",
    "#Reduce learning Rate\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.01,\n",
    "                              patience = 6,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.001)\n",
    "\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callback = [earlystop, checkpoint,reduce_lr]\n",
    "\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=callback,epochs=220,batch_size=32,verbose=1)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "\n",
    "\n",
    "### Lets Do the prediction and check performance metrics\n",
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test)\n",
    "train_predict.shape, test_predict.shape\n",
    "\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "# Evaluation metrices RMSE and MAE\n",
    "print(\"Train data RMSE: \", math.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
    "print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
    "print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Test data RMSE: \", math.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
    "print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
    "print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))\n",
    "\n",
    "print(\"Train data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytrain, train_predict))\n",
    "print(\"Test data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytest, test_predict))\n",
    "\n",
    "\n",
    "print(\"Train data R2 score:\", r2_score(original_ytrain, train_predict))\n",
    "print(\"Test data R2 score:\", r2_score(original_ytest, test_predict))\n",
    "\n",
    "print(\"Train data MGD: \", mean_gamma_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MGD: \", mean_gamma_deviance(original_ytest, test_predict))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Train data MPD: \", mean_poisson_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MPD: \", mean_poisson_deviance(original_ytest, test_predict))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "look_back=time_step\n",
    "trainPredictPlot = np.empty_like(closedf)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "print(\"Train predicted data: \", trainPredictPlot.shape)\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(closedf)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(closedf)-1, :] = test_predict\n",
    "print(\"Test predicted data: \", testPredictPlot.shape)\n",
    "\n",
    "\n",
    "x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
    "temp_input=list(x_input)\n",
    "temp_input=temp_input[0].tolist()\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "lst_output=[]\n",
    "n_steps=time_step\n",
    "i=0\n",
    "pred_days = 30    # Prediction for next 30 days\n",
    "\n",
    "while(i<pred_days):\n",
    "\n",
    "    if(len(temp_input)>time_step):\n",
    "\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        #print(\"{} day input {}\".format(i,x_input))\n",
    "        x_input = x_input.reshape(1,-1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        #print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "    else:\n",
    "\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "print(\"Output of predicted next days: \", len(lst_output))\n",
    "\n",
    "last_days=np.arange(1,time_step+1)\n",
    "day_pred=np.arange(time_step+1,time_step+pred_days+1)\n",
    "print(last_days)\n",
    "print(day_pred)\n",
    "\n",
    "temp_mat = np.empty((len(last_days)+pred_days+1,1))\n",
    "temp_mat[:] = np.nan\n",
    "temp_mat = temp_mat.reshape(1,-1).tolist()[0]\n",
    "\n",
    "last_original_days_value = temp_mat\n",
    "next_predicted_days_value = temp_mat\n",
    "\n",
    "last_original_days_value[0:time_step+1] = scaler.inverse_transform(closedf[len(closedf)-time_step:]).reshape(1,-1).tolist()[0]\n",
    "next_predicted_days_value[time_step+1:] = scaler.inverse_transform(np.array(lst_output).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "\n",
    "new_pred_plot = pd.DataFrame({\n",
    "    'last_original_days_value':last_original_days_value,\n",
    "    'next_predicted_days_value':next_predicted_days_value\n",
    "})\n",
    "\n",
    "names = cycle(['Last 15 days close price','Predicted next 30 days close price'])\n",
    "\n",
    "\n",
    "# Assuming you have new_pred_plot DataFrame with necessary columns\n",
    "\n",
    "\n",
    "\n",
    "lstmdf=closedf.tolist()\n",
    "lstmdf.extend((np.array(lst_output).reshape(-1,1)).tolist())\n",
    "lstmdf=scaler.inverse_transform(lstmdf).reshape(1,-1).tolist()[0]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have lstmdf list of stock prices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(next_predicted_days_value[:30])\n",
    "print(len(next_predicted_days_value))\n",
    "current = df.iloc[len(df)-1]\n",
    "yesterday = df.iloc[len(df)-2]\n",
    "\n",
    "predictedprices=next_predicted_days_value[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abdf1727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of close dataframe: (3604, 2)\n",
      "(3604, 1)\n",
      "Epoch 1/220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itssh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028\n",
      "Epoch 1: val_loss improved from inf to 0.00226, saving model to E:/cryptics/dataset/best_model_banana.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0028 - val_loss: 0.0023 - learning_rate: 0.0010\n",
      "Epoch 2/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0018\n",
      "Epoch 2: val_loss improved from 0.00226 to 0.00171, saving model to E:/cryptics/dataset/best_model_banana.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0018 - val_loss: 0.0017 - learning_rate: 0.0010\n",
      "Epoch 3/220\n",
      "\u001b[1m75/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.5394e-04\n",
      "Epoch 3: val_loss improved from 0.00171 to 0.00158, saving model to E:/cryptics/dataset/best_model_banana.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 3.8371e-04 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 4/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0016\n",
      "Epoch 4: val_loss did not improve from 0.00158\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0016 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 5/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.8325e-04\n",
      "Epoch 5: val_loss did not improve from 0.00158\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 4.9419e-04 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 6/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.4603e-04\n",
      "Epoch 6: val_loss improved from 0.00158 to 0.00144, saving model to E:/cryptics/dataset/best_model_banana.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 6.5626e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 7/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.7353e-04\n",
      "Epoch 7: val_loss did not improve from 0.00144\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 8.7473e-04 - val_loss: 0.0020 - learning_rate: 0.0010\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Train data RMSE:  15.096286862377164\n",
      "Train data MSE:  227.89787703118137\n",
      "Train data MAE:  5.707625639068223\n",
      "-------------------------------------------------------------------------------------\n",
      "Test data RMSE:  22.93457899472153\n",
      "Test data MSE:  525.994913665122\n",
      "Test data MAE:  19.661405512467663\n",
      "Train data explained variance regression score: 0.43589279669345427\n",
      "Test data explained variance regression score: 0.6849771468357271\n",
      "Train data R2 score: 0.41211899817481956\n",
      "Test data R2 score: -0.1090912303527456\n",
      "Train data MGD:  0.015731589775394978\n",
      "Test data MGD:  0.033013265321442074\n",
      "----------------------------------------------------------------------\n",
      "Train data MPD:  1.6811993635344704\n",
      "Test data MPD:  4.145370607276171\n",
      "Train predicted data:  (3604, 1)\n",
      "Test predicted data:  (3604, 1)\n",
      "Output of predicted next days:  30\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "[31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59 60]\n",
      "[130.0, 130.0, 130.0, 130.0, 130.0, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 130.0, 130.0, 130.0, 130.0, 130.0, 140.0, 145.0, 136.67, 135.0, 136.67, 125.00000000000001, 125.00000000000001, 130.0, 130.0, 130.0, 130.0]\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('E:/cryptics/dataset/Banana.csv')\n",
    "df\n",
    "closedf = df[['sn','price']]\n",
    "print(\"Shape of close dataframe:\", closedf.shape)\n",
    "del closedf['sn']\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "closedf=scaler.fit_transform(np.array(closedf).reshape(-1,1))\n",
    "print(closedf.shape)\n",
    "\n",
    "training_size=int(len(closedf)*0.70)\n",
    "test_size=len(closedf)-training_size\n",
    "train_data,test_data=closedf[0:training_size,:],closedf[training_size:len(closedf),:1]\n",
    "\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "time_step = 30\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "\n",
    "#red potato\n",
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=(None,1),activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "\n",
    "#early stopping and model checking\n",
    "\n",
    "#Early Stopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', #value being monitored for improvement\n",
    "                          min_delta = 0.001,         #Abs value and is the main change required before we stop\n",
    "                          mode='auto',\n",
    "                          patience = 6,             #no of epocs we wait before stopping\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keep the best weigts once stopped\n",
    "#Model Checkpoint\n",
    "checkpoint = ModelCheckpoint(monitor = \"val_loss\",\n",
    "                             mode ='auto',\n",
    "                             filepath = \"E:/cryptics/dataset/best_model_banana.keras\",\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             )\n",
    "\n",
    "#Reduce learning Rate\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.01,\n",
    "                              patience = 6,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.001)\n",
    "\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callback = [earlystop, checkpoint,reduce_lr]\n",
    "\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=callback,epochs=220,batch_size=32,verbose=1)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "\n",
    "\n",
    "### Lets Do the prediction and check performance metrics\n",
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test)\n",
    "train_predict.shape, test_predict.shape\n",
    "\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "# Evaluation metrices RMSE and MAE\n",
    "print(\"Train data RMSE: \", math.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
    "print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
    "print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Test data RMSE: \", math.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
    "print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
    "print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))\n",
    "\n",
    "print(\"Train data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytrain, train_predict))\n",
    "print(\"Test data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytest, test_predict))\n",
    "\n",
    "\n",
    "print(\"Train data R2 score:\", r2_score(original_ytrain, train_predict))\n",
    "print(\"Test data R2 score:\", r2_score(original_ytest, test_predict))\n",
    "\n",
    "print(\"Train data MGD: \", mean_gamma_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MGD: \", mean_gamma_deviance(original_ytest, test_predict))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Train data MPD: \", mean_poisson_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MPD: \", mean_poisson_deviance(original_ytest, test_predict))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "look_back=time_step\n",
    "trainPredictPlot = np.empty_like(closedf)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "print(\"Train predicted data: \", trainPredictPlot.shape)\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(closedf)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(closedf)-1, :] = test_predict\n",
    "print(\"Test predicted data: \", testPredictPlot.shape)\n",
    "\n",
    "\n",
    "x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
    "temp_input=list(x_input)\n",
    "temp_input=temp_input[0].tolist()\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "lst_output=[]\n",
    "n_steps=time_step\n",
    "i=0\n",
    "pred_days = 30    # Prediction for next 30 days\n",
    "\n",
    "while(i<pred_days):\n",
    "\n",
    "    if(len(temp_input)>time_step):\n",
    "\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        #print(\"{} day input {}\".format(i,x_input))\n",
    "        x_input = x_input.reshape(1,-1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        #print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "    else:\n",
    "\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "print(\"Output of predicted next days: \", len(lst_output))\n",
    "\n",
    "last_days=np.arange(1,time_step+1)\n",
    "day_pred=np.arange(time_step+1,time_step+pred_days+1)\n",
    "print(last_days)\n",
    "print(day_pred)\n",
    "\n",
    "temp_mat = np.empty((len(last_days)+pred_days+1,1))\n",
    "temp_mat[:] = np.nan\n",
    "temp_mat = temp_mat.reshape(1,-1).tolist()[0]\n",
    "\n",
    "last_original_days_value = temp_mat\n",
    "next_predicted_days_value = temp_mat\n",
    "\n",
    "last_original_days_value[0:time_step+1] = scaler.inverse_transform(closedf[len(closedf)-time_step:]).reshape(1,-1).tolist()[0]\n",
    "next_predicted_days_value[time_step+1:] = scaler.inverse_transform(np.array(lst_output).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "\n",
    "new_pred_plot = pd.DataFrame({\n",
    "    'last_original_days_value':last_original_days_value,\n",
    "    'next_predicted_days_value':next_predicted_days_value\n",
    "})\n",
    "\n",
    "names = cycle(['Last 15 days close price','Predicted next 30 days close price'])\n",
    "\n",
    "\n",
    "# Assuming you have new_pred_plot DataFrame with necessary columns\n",
    "\n",
    "\n",
    "\n",
    "lstmdf=closedf.tolist()\n",
    "lstmdf.extend((np.array(lst_output).reshape(-1,1)).tolist())\n",
    "lstmdf=scaler.inverse_transform(lstmdf).reshape(1,-1).tolist()[0]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have lstmdf list of stock prices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(next_predicted_days_value[:30])\n",
    "print(len(next_predicted_days_value))\n",
    "current1 = df.iloc[len(df)-1]\n",
    "yesterday1 = df.iloc[len(df)-2]\n",
    "\n",
    "predictedprices1=next_predicted_days_value[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377aa824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.0, 28.999999999999996, 30.0, 30.0, 28.999999999999996, 28.999999999999996, 28.999999999999996, 29.25, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 31.0, 31.0, 32.5, 32.0, 32.0, 34.0, 34.0, 33.8, 33.33, 34.0, 33.6, 33.4, 33.5, 33.0, 33.5]\n",
      "[130.0, 130.0, 130.0, 130.0, 130.0, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 130.0, 130.0, 130.0, 130.0, 130.0, 140.0, 145.0, 136.67, 135.0, 136.67, 125.00000000000001, 125.00000000000001, 130.0, 130.0, 130.0, 130.0]\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:27333\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_9356\\4260284247.py:16: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_9356\\4260284247.py:17: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "127.0.0.1 - - [10/Mar/2024 22:13:22] \"GET /getPotatoValues HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "print(predictedprices)\n",
    "print(predictedprices1)\n",
    "\n",
    "@app.route('/getPotatoValues', methods=['GET'])\n",
    "def get_potato_values():\n",
    "    # Ensure there are at least 3 values in the predicted_prices array\n",
    "    if len(predictedprices) >= 3:\n",
    "        potato_data = {\n",
    "            \"name\": \"potato\",\n",
    "            \"currentPrice\": current[-1],\n",
    "            \"yesterdayPrice\": yesterday[-1],\n",
    "            \"tomorrowPrice\": predictedprices[0],\n",
    "            \"description\": \"This is potato\",\n",
    "            \"locality\": \"Nepali\",\n",
    "        }\n",
    "        \n",
    "        banana_data = {\n",
    "            \"name\": \"banana\",\n",
    "            \"currentPrice\": current1[-1],\n",
    "            \"yesterdayPrice\": yesterday1[-1],\n",
    "            \"tomorrowPrice\": predictedprices1[0],\n",
    "            \"description\": \"This is Banana\",\n",
    "            \"locality\": \"Nepali\",\n",
    "        }\n",
    "        return jsonify(potato_data,banana_data)\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Not enough data available\"}), 404\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(port=27333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e244355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"next\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
