{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebadc2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.29.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First we will import the necessary Library\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime as dt\n",
    "\n",
    "# For Evalution we will use these library\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For model building we will use these library\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "# For PLotting we will use these library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "%matplotlib inline\n",
    "from plotly.offline import init_notebook_mode,iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd6743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of close dataframe: (3595, 2)\n",
      "(3595, 1)\n",
      "Epoch 1/220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itssh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0152\n",
      "Epoch 1: val_loss improved from inf to 0.00217, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - loss: 0.0150 - val_loss: 0.0022 - learning_rate: 0.0010\n",
      "Epoch 2/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013\n",
      "Epoch 2: val_loss improved from 0.00217 to 0.00188, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 3/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0011\n",
      "Epoch 3: val_loss improved from 0.00188 to 0.00107, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0011 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 4/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.8906e-04\n",
      "Epoch 4: val_loss improved from 0.00107 to 0.00095, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 8.9050e-04 - val_loss: 9.4998e-04 - learning_rate: 0.0010\n",
      "Epoch 5/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 8.7191e-04\n",
      "Epoch 5: val_loss did not improve from 0.00095\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 8.7087e-04 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 6/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.5133e-04\n",
      "Epoch 6: val_loss improved from 0.00095 to 0.00085, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7.5125e-04 - val_loss: 8.4599e-04 - learning_rate: 0.0010\n",
      "Epoch 7/220\n",
      "\u001b[1m74/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.6065e-04\n",
      "Epoch 7: val_loss improved from 0.00085 to 0.00071, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 6.6242e-04 - val_loss: 7.1335e-04 - learning_rate: 0.0010\n",
      "Epoch 8/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.7589e-04\n",
      "Epoch 8: val_loss improved from 0.00071 to 0.00069, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.7677e-04 - val_loss: 6.8744e-04 - learning_rate: 0.0010\n",
      "Epoch 9/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.3560e-04\n",
      "Epoch 9: val_loss improved from 0.00069 to 0.00064, saving model to E:/cryptics/dataset/best_model.keras\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 6.3390e-04 - val_loss: 6.4442e-04 - learning_rate: 0.0010\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Train data RMSE:  2.9172508744153576\n",
      "Train data MSE:  8.51035266427717\n",
      "Train data MAE:  1.7984058504315692\n",
      "-------------------------------------------------------------------------------------\n",
      "Test data RMSE:  3.242619058291528\n",
      "Test data MSE:  10.514578357195434\n",
      "Test data MAE:  2.1051352554059215\n",
      "Train data explained variance regression score: 0.966281293675516\n",
      "Test data explained variance regression score: 0.9542916188891031\n",
      "Train data R2 score: 0.9655834922908044\n",
      "Test data R2 score: 0.9536521751702821\n",
      "Train data MGD:  0.0049613590499551045\n",
      "Test data MGD:  0.004225229227832882\n",
      "----------------------------------------------------------------------\n",
      "Train data MPD:  0.18875056824047484\n",
      "Test data MPD:  0.199277408200627\n",
      "Train predicted data:  (3595, 1)\n",
      "Test predicted data:  (3595, 1)\n",
      "Output of predicted next days:  30\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "[31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59 60]\n",
      "[31.0, 28.999999999999996, 30.0, 30.0, 28.999999999999996, 28.999999999999996, 28.999999999999996, 29.25, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 31.0, 31.0, 32.5, 32.0, 32.0, 34.0, 34.0, 33.8, 33.33, 34.0, 33.6, 33.4, 33.5, 33.0, 33.5]\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('E:/cryptics/dataset/Potato_Red.csv')\n",
    "df\n",
    "closedf = df[['sn','price']]\n",
    "print(\"Shape of close dataframe:\", closedf.shape)\n",
    "del closedf['sn']\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "closedf=scaler.fit_transform(np.array(closedf).reshape(-1,1))\n",
    "print(closedf.shape)\n",
    "\n",
    "training_size=int(len(closedf)*0.70)\n",
    "test_size=len(closedf)-training_size\n",
    "train_data,test_data=closedf[0:training_size,:],closedf[training_size:len(closedf),:1]\n",
    "\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "time_step = 30\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "\n",
    "#red potato\n",
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=(None,1),activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "\n",
    "#early stopping and model checking\n",
    "\n",
    "#Early Stopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', #value being monitored for improvement\n",
    "                          min_delta = 0.001,         #Abs value and is the main change required before we stop\n",
    "                          mode='auto',\n",
    "                          patience = 6,             #no of epocs we wait before stopping\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keep the best weigts once stopped\n",
    "#Model Checkpoint\n",
    "checkpoint = ModelCheckpoint(monitor = \"val_loss\",\n",
    "                             mode ='auto',\n",
    "                             filepath = \"E:/cryptics/dataset/best_model.keras\",\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             )\n",
    "\n",
    "#Reduce learning Rate\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.01,\n",
    "                              patience = 6,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.001)\n",
    "\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callback = [earlystop, checkpoint,reduce_lr]\n",
    "\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=callback,epochs=220,batch_size=32,verbose=1)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "\n",
    "\n",
    "### Lets Do the prediction and check performance metrics\n",
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test)\n",
    "train_predict.shape, test_predict.shape\n",
    "\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "# Evaluation metrices RMSE and MAE\n",
    "print(\"Train data RMSE: \", math.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
    "print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
    "print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Test data RMSE: \", math.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
    "print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
    "print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))\n",
    "\n",
    "print(\"Train data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytrain, train_predict))\n",
    "print(\"Test data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytest, test_predict))\n",
    "\n",
    "\n",
    "print(\"Train data R2 score:\", r2_score(original_ytrain, train_predict))\n",
    "print(\"Test data R2 score:\", r2_score(original_ytest, test_predict))\n",
    "\n",
    "print(\"Train data MGD: \", mean_gamma_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MGD: \", mean_gamma_deviance(original_ytest, test_predict))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Train data MPD: \", mean_poisson_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MPD: \", mean_poisson_deviance(original_ytest, test_predict))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "look_back=time_step\n",
    "trainPredictPlot = np.empty_like(closedf)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "print(\"Train predicted data: \", trainPredictPlot.shape)\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(closedf)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(closedf)-1, :] = test_predict\n",
    "print(\"Test predicted data: \", testPredictPlot.shape)\n",
    "\n",
    "\n",
    "x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
    "temp_input=list(x_input)\n",
    "temp_input=temp_input[0].tolist()\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "lst_output=[]\n",
    "n_steps=time_step\n",
    "i=0\n",
    "pred_days = 30    # Prediction for next 30 days\n",
    "\n",
    "while(i<pred_days):\n",
    "\n",
    "    if(len(temp_input)>time_step):\n",
    "\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        #print(\"{} day input {}\".format(i,x_input))\n",
    "        x_input = x_input.reshape(1,-1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        #print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "    else:\n",
    "\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "print(\"Output of predicted next days: \", len(lst_output))\n",
    "\n",
    "last_days=np.arange(1,time_step+1)\n",
    "day_pred=np.arange(time_step+1,time_step+pred_days+1)\n",
    "print(last_days)\n",
    "print(day_pred)\n",
    "\n",
    "temp_mat = np.empty((len(last_days)+pred_days+1,1))\n",
    "temp_mat[:] = np.nan\n",
    "temp_mat = temp_mat.reshape(1,-1).tolist()[0]\n",
    "\n",
    "last_original_days_value = temp_mat\n",
    "next_predicted_days_value = temp_mat\n",
    "\n",
    "last_original_days_value[0:time_step+1] = scaler.inverse_transform(closedf[len(closedf)-time_step:]).reshape(1,-1).tolist()[0]\n",
    "next_predicted_days_value[time_step+1:] = scaler.inverse_transform(np.array(lst_output).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "\n",
    "new_pred_plot = pd.DataFrame({\n",
    "    'last_original_days_value':last_original_days_value,\n",
    "    'next_predicted_days_value':next_predicted_days_value\n",
    "})\n",
    "\n",
    "names = cycle(['Last 15 days close price','Predicted next 30 days close price'])\n",
    "\n",
    "\n",
    "# Assuming you have new_pred_plot DataFrame with necessary columns\n",
    "\n",
    "\n",
    "\n",
    "lstmdf=closedf.tolist()\n",
    "lstmdf.extend((np.array(lst_output).reshape(-1,1)).tolist())\n",
    "lstmdf=scaler.inverse_transform(lstmdf).reshape(1,-1).tolist()[0]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have lstmdf list of stock prices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(next_predicted_days_value[:30])\n",
    "print(len(next_predicted_days_value))\n",
    "current =df.iloc[len(df)-1]\n",
    "yesterday = df.iloc[len(df)-2]\n",
    "\n",
    "predictedprices=next_predicted_days_value[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abdf1727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of close dataframe: (3604, 2)\n",
      "(3604, 1)\n",
      "Epoch 1/220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itssh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0018\n",
      "Epoch 1: val_loss improved from inf to 0.00083, saving model to E:/cryptics/dataset/best_model_banana.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 0.0018 - val_loss: 8.2963e-04 - learning_rate: 0.0010\n",
      "Epoch 2/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0014\n",
      "Epoch 2: val_loss did not improve from 0.00083\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 3/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0010\n",
      "Epoch 3: val_loss did not improve from 0.00083\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0010 - val_loss: 0.0013 - learning_rate: 0.0010\n",
      "Epoch 4/220\n",
      "\u001b[1m75/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.4400e-04\n",
      "Epoch 4: val_loss did not improve from 0.00083\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 8.4772e-04 - val_loss: 0.0014 - learning_rate: 0.0010\n",
      "Epoch 5/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014\n",
      "Epoch 5: val_loss did not improve from 0.00083\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0012 - learning_rate: 0.0010\n",
      "Epoch 6/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0014\n",
      "Epoch 6: val_loss did not improve from 0.00083\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0013 - val_loss: 0.0016 - learning_rate: 0.0010\n",
      "Epoch 7/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.8302e-04\n",
      "Epoch 7: val_loss did not improve from 0.00083\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 5.8716e-04 - val_loss: 0.0018 - learning_rate: 0.0010\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Train data RMSE:  15.945384220959617\n",
      "Train data MSE:  254.25527795402795\n",
      "Train data MAE:  8.393141696398539\n",
      "-------------------------------------------------------------------------------------\n",
      "Test data RMSE:  13.898538726455662\n",
      "Test data MSE:  193.16937873078777\n",
      "Test data MAE:  10.789138572975979\n",
      "Train data explained variance regression score: 0.4418722608597879\n",
      "Test data explained variance regression score: 0.7613876301290085\n",
      "Train data R2 score: 0.34412795121166184\n",
      "Test data R2 score: 0.5926909968954496\n",
      "Train data MGD:  0.021456209856603296\n",
      "Test data MGD:  0.01055824460766938\n",
      "----------------------------------------------------------------------\n",
      "Train data MPD:  2.0648506972282847\n",
      "Test data MPD:  1.4197914874264412\n",
      "Train predicted data:  (3604, 1)\n",
      "Test predicted data:  (3604, 1)\n",
      "Output of predicted next days:  30\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "[31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59 60]\n",
      "[130.0, 130.0, 130.0, 130.0, 130.0, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 130.0, 130.0, 130.0, 130.0, 130.0, 140.0, 145.0, 136.67, 135.0, 136.67, 125.00000000000001, 125.00000000000001, 130.0, 130.0, 130.0, 130.0]\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('E:/cryptics/dataset/Banana.csv')\n",
    "df\n",
    "closedf = df[['sn','price']]\n",
    "print(\"Shape of close dataframe:\", closedf.shape)\n",
    "del closedf['sn']\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "closedf=scaler.fit_transform(np.array(closedf).reshape(-1,1))\n",
    "print(closedf.shape)\n",
    "\n",
    "training_size=int(len(closedf)*0.70)\n",
    "test_size=len(closedf)-training_size\n",
    "train_data,test_data=closedf[0:training_size,:],closedf[training_size:len(closedf),:1]\n",
    "\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "time_step = 30\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "\n",
    "#red potato\n",
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=(None,1),activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "\n",
    "#early stopping and model checking\n",
    "\n",
    "#Early Stopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', #value being monitored for improvement\n",
    "                          min_delta = 0.001,         #Abs value and is the main change required before we stop\n",
    "                          mode='auto',\n",
    "                          patience = 6,             #no of epocs we wait before stopping\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keep the best weigts once stopped\n",
    "#Model Checkpoint\n",
    "checkpoint = ModelCheckpoint(monitor = \"val_loss\",\n",
    "                             mode ='auto',\n",
    "                             filepath = \"E:/cryptics/dataset/best_model_banana.keras\",\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             )\n",
    "\n",
    "#Reduce learning Rate\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.01,\n",
    "                              patience = 6,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.001)\n",
    "\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callback = [earlystop, checkpoint,reduce_lr]\n",
    "\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=callback,epochs=220,batch_size=32,verbose=1)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "\n",
    "\n",
    "### Lets Do the prediction and check performance metrics\n",
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test)\n",
    "train_predict.shape, test_predict.shape\n",
    "\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "# Evaluation metrices RMSE and MAE\n",
    "print(\"Train data RMSE: \", math.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
    "print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
    "print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Test data RMSE: \", math.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
    "print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
    "print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))\n",
    "\n",
    "print(\"Train data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytrain, train_predict))\n",
    "print(\"Test data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytest, test_predict))\n",
    "\n",
    "\n",
    "print(\"Train data R2 score:\", r2_score(original_ytrain, train_predict))\n",
    "print(\"Test data R2 score:\", r2_score(original_ytest, test_predict))\n",
    "\n",
    "print(\"Train data MGD: \", mean_gamma_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MGD: \", mean_gamma_deviance(original_ytest, test_predict))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Train data MPD: \", mean_poisson_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MPD: \", mean_poisson_deviance(original_ytest, test_predict))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "look_back=time_step\n",
    "trainPredictPlot = np.empty_like(closedf)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "print(\"Train predicted data: \", trainPredictPlot.shape)\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(closedf)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(closedf)-1, :] = test_predict\n",
    "print(\"Test predicted data: \", testPredictPlot.shape)\n",
    "\n",
    "\n",
    "x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
    "temp_input=list(x_input)\n",
    "temp_input=temp_input[0].tolist()\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "lst_output=[]\n",
    "n_steps=time_step\n",
    "i=0\n",
    "pred_days = 30    # Prediction for next 30 days\n",
    "\n",
    "while(i<pred_days):\n",
    "\n",
    "    if(len(temp_input)>time_step):\n",
    "\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        #print(\"{} day input {}\".format(i,x_input))\n",
    "        x_input = x_input.reshape(1,-1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        #print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "    else:\n",
    "\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "print(\"Output of predicted next days: \", len(lst_output))\n",
    "\n",
    "last_days=np.arange(1,time_step+1)\n",
    "day_pred=np.arange(time_step+1,time_step+pred_days+1)\n",
    "print(last_days)\n",
    "print(day_pred)\n",
    "\n",
    "temp_mat = np.empty((len(last_days)+pred_days+1,1))\n",
    "temp_mat[:] = np.nan\n",
    "temp_mat = temp_mat.reshape(1,-1).tolist()[0]\n",
    "\n",
    "last_original_days_value = temp_mat\n",
    "next_predicted_days_value = temp_mat\n",
    "\n",
    "last_original_days_value[0:time_step+1] = scaler.inverse_transform(closedf[len(closedf)-time_step:]).reshape(1,-1).tolist()[0]\n",
    "next_predicted_days_value[time_step+1:] = scaler.inverse_transform(np.array(lst_output).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "\n",
    "new_pred_plot = pd.DataFrame({\n",
    "    'last_original_days_value':last_original_days_value,\n",
    "    'next_predicted_days_value':next_predicted_days_value\n",
    "})\n",
    "\n",
    "names = cycle(['Last 15 days close price','Predicted next 30 days close price'])\n",
    "\n",
    "\n",
    "# Assuming you have new_pred_plot DataFrame with necessary columns\n",
    "\n",
    "\n",
    "\n",
    "lstmdf=closedf.tolist()\n",
    "lstmdf.extend((np.array(lst_output).reshape(-1,1)).tolist())\n",
    "lstmdf=scaler.inverse_transform(lstmdf).reshape(1,-1).tolist()[0]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have lstmdf list of stock prices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(next_predicted_days_value[:30])\n",
    "print(len(next_predicted_days_value))\n",
    "current1 =df.iloc[len(df)-1]\n",
    "yesterday1 = df.iloc[len(df)-2]\n",
    "\n",
    "predictedprices1=next_predicted_days_value[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d26eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of close dataframe: (3611, 2)\n",
      "(3611, 1)\n",
      "Epoch 1/220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itssh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0154\n",
      "Epoch 1: val_loss improved from inf to 0.00521, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - loss: 0.0151 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 2/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0028\n",
      "Epoch 2: val_loss improved from 0.00521 to 0.00478, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0028 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 3/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024\n",
      "Epoch 3: val_loss improved from 0.00478 to 0.00421, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0024 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 4/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020\n",
      "Epoch 4: val_loss improved from 0.00421 to 0.00392, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 5/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0020\n",
      "Epoch 5: val_loss improved from 0.00392 to 0.00377, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0020 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 6/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020\n",
      "Epoch 6: val_loss improved from 0.00377 to 0.00344, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0034 - learning_rate: 0.0010\n",
      "Epoch 7/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0019\n",
      "Epoch 7: val_loss improved from 0.00344 to 0.00332, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0033 - learning_rate: 0.0010\n",
      "Epoch 8/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0017\n",
      "Epoch 8: val_loss did not improve from 0.00332\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 9/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0018\n",
      "Epoch 9: val_loss improved from 0.00332 to 0.00317, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0018 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 10/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017\n",
      "Epoch 10: val_loss improved from 0.00317 to 0.00309, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 11/220\n",
      "\u001b[1m75/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017\n",
      "Epoch 11: val_loss improved from 0.00309 to 0.00301, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0017 - val_loss: 0.0030 - learning_rate: 0.0010\n",
      "Epoch 12/220\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0015\n",
      "Epoch 12: val_loss improved from 0.00301 to 0.00292, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 13/220\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0017\n",
      "Epoch 13: val_loss did not improve from 0.00292\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0017 - val_loss: 0.0032 - learning_rate: 0.0010\n",
      "Epoch 14/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0017\n",
      "Epoch 14: val_loss did not improve from 0.00292\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0017 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 15/220\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0016\n",
      "Epoch 15: val_loss improved from 0.00292 to 0.00290, saving model to E:/cryptics/dataset/best_model_cauli.keras\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "Train data RMSE:  8.265676605166513\n",
      "Train data MSE:  68.32140974119702\n",
      "Train data MAE:  5.33873147655756\n",
      "-------------------------------------------------------------------------------------\n",
      "Test data RMSE:  11.146357463583769\n",
      "Test data MSE:  124.24128470598959\n",
      "Test data MAE:  7.406663798362101\n",
      "Train data explained variance regression score: 0.9046171286865266\n",
      "Test data explained variance regression score: 0.8379344349474949\n",
      "Train data R2 score: 0.9012293291019284\n",
      "Test data R2 score: 0.8327135914514594\n",
      "Train data MGD:  0.026844344296749874\n",
      "Test data MGD:  0.03327322260017473\n",
      "----------------------------------------------------------------------\n",
      "Train data MPD:  1.170215724489043\n",
      "Test data MPD:  1.812918585640563\n",
      "Train predicted data:  (3611, 1)\n",
      "Test predicted data:  (3611, 1)\n",
      "Output of predicted next days:  30\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "[31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59 60]\n",
      "[45.0, 35.0, 35.0, 35.0, 30.0, 27.670000000000005, 25.750000000000004, 32.67, 28.25, 27.670000000000005, 29.5, 28.999999999999996, 31.0, 27.670000000000005, 28.999999999999996, 30.0, 28.999999999999996, 30.0, 30.0, 30.0, 30.0, 27.000000000000004, 27.670000000000005, 27.5, 30.0, 30.0, 35.0, 35.0, 40.0, 45.0]\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('E:/cryptics/dataset/Cauli_Local.csv')\n",
    "df\n",
    "closedf = df[['sn','price']]\n",
    "print(\"Shape of close dataframe:\", closedf.shape)\n",
    "del closedf['sn']\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "closedf=scaler.fit_transform(np.array(closedf).reshape(-1,1))\n",
    "print(closedf.shape)\n",
    "\n",
    "training_size=int(len(closedf)*0.70)\n",
    "test_size=len(closedf)-training_size\n",
    "train_data,test_data=closedf[0:training_size,:],closedf[training_size:len(closedf),:1]\n",
    "\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "time_step = 30\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "\n",
    "#red potato\n",
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=(None,1),activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "\n",
    "#early stopping and model checking\n",
    "\n",
    "#Early Stopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', #value being monitored for improvement\n",
    "                          min_delta = 0.001,         #Abs value and is the main change required before we stop\n",
    "                          mode='auto',\n",
    "                          patience = 6,             #no of epocs we wait before stopping\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keep the best weigts once stopped\n",
    "#Model Checkpoint\n",
    "checkpoint = ModelCheckpoint(monitor = \"val_loss\",\n",
    "                             mode ='auto',\n",
    "                             filepath = \"E:/cryptics/dataset/best_model_cauli.keras\",\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             )\n",
    "\n",
    "#Reduce learning Rate\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.01,\n",
    "                              patience = 6,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.001)\n",
    "\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callback = [earlystop, checkpoint,reduce_lr]\n",
    "\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=callback,epochs=220,batch_size=32,verbose=1)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "\n",
    "\n",
    "### Lets Do the prediction and check performance metrics\n",
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test)\n",
    "train_predict.shape, test_predict.shape\n",
    "\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "# Evaluation metrices RMSE and MAE\n",
    "print(\"Train data RMSE: \", math.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
    "print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
    "print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Test data RMSE: \", math.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
    "print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
    "print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))\n",
    "\n",
    "print(\"Train data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytrain, train_predict))\n",
    "print(\"Test data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytest, test_predict))\n",
    "\n",
    "\n",
    "print(\"Train data R2 score:\", r2_score(original_ytrain, train_predict))\n",
    "print(\"Test data R2 score:\", r2_score(original_ytest, test_predict))\n",
    "\n",
    "print(\"Train data MGD: \", mean_gamma_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MGD: \", mean_gamma_deviance(original_ytest, test_predict))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Train data MPD: \", mean_poisson_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MPD: \", mean_poisson_deviance(original_ytest, test_predict))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "look_back=time_step\n",
    "trainPredictPlot = np.empty_like(closedf)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "print(\"Train predicted data: \", trainPredictPlot.shape)\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(closedf)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(closedf)-1, :] = test_predict\n",
    "print(\"Test predicted data: \", testPredictPlot.shape)\n",
    "\n",
    "\n",
    "x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
    "temp_input=list(x_input)\n",
    "temp_input=temp_input[0].tolist()\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "lst_output=[]\n",
    "n_steps=time_step\n",
    "i=0\n",
    "pred_days = 30    # Prediction for next 30 days\n",
    "\n",
    "while(i<pred_days):\n",
    "\n",
    "    if(len(temp_input)>time_step):\n",
    "\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        #print(\"{} day input {}\".format(i,x_input))\n",
    "        x_input = x_input.reshape(1,-1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        #print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "    else:\n",
    "\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "print(\"Output of predicted next days: \", len(lst_output))\n",
    "\n",
    "last_days=np.arange(1,time_step+1)\n",
    "day_pred=np.arange(time_step+1,time_step+pred_days+1)\n",
    "print(last_days)\n",
    "print(day_pred)\n",
    "\n",
    "temp_mat = np.empty((len(last_days)+pred_days+1,1))\n",
    "temp_mat[:] = np.nan\n",
    "temp_mat = temp_mat.reshape(1,-1).tolist()[0]\n",
    "\n",
    "last_original_days_value = temp_mat\n",
    "next_predicted_days_value = temp_mat\n",
    "\n",
    "last_original_days_value[0:time_step+1] = scaler.inverse_transform(closedf[len(closedf)-time_step:]).reshape(1,-1).tolist()[0]\n",
    "next_predicted_days_value[time_step+1:] = scaler.inverse_transform(np.array(lst_output).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "\n",
    "new_pred_plot = pd.DataFrame({\n",
    "    'last_original_days_value':last_original_days_value,\n",
    "    'next_predicted_days_value':next_predicted_days_value\n",
    "})\n",
    "\n",
    "names = cycle(['Last 15 days close price','Predicted next 30 days close price'])\n",
    "\n",
    "\n",
    "# Assuming you have new_pred_plot DataFrame with necessary columns\n",
    "\n",
    "\n",
    "\n",
    "lstmdf=closedf.tolist()\n",
    "lstmdf.extend((np.array(lst_output).reshape(-1,1)).tolist())\n",
    "lstmdf=scaler.inverse_transform(lstmdf).reshape(1,-1).tolist()[0]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have lstmdf list of stock prices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(next_predicted_days_value[:30])\n",
    "print(len(next_predicted_days_value))\n",
    "current2 = df.iloc[len(df)-1]\n",
    "yesterday2 = df.iloc[len(df)-2]\n",
    "\n",
    "predictedprices2=next_predicted_days_value[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4301579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of close dataframe: (3159, 2)\n",
      "(3159, 1)\n",
      "Epoch 1/220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\itssh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0190\n",
      "Epoch 1: val_loss improved from inf to 0.00545, saving model to E:/cryptics/dataset/best_model_tomato.keras\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0182 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 2/220\n",
      "\u001b[1m67/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028\n",
      "Epoch 2: val_loss improved from 0.00545 to 0.00474, saving model to E:/cryptics/dataset/best_model_tomato.keras\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0028 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 3/220\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0025\n",
      "Epoch 3: val_loss improved from 0.00474 to 0.00380, saving model to E:/cryptics/dataset/best_model_tomato.keras\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0025 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 4/220\n",
      "\u001b[1m67/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0023\n",
      "Epoch 4: val_loss did not improve from 0.00380\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0023 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 5/220\n",
      "\u001b[1m67/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0022\n",
      "Epoch 5: val_loss improved from 0.00380 to 0.00376, saving model to E:/cryptics/dataset/best_model_tomato.keras\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0022 - val_loss: 0.0038 - learning_rate: 0.0010\n",
      "Epoch 6/220\n",
      "\u001b[1m68/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0019\n",
      "Epoch 6: val_loss improved from 0.00376 to 0.00312, saving model to E:/cryptics/dataset/best_model_tomato.keras\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step - loss: 0.0019 - val_loss: 0.0031 - learning_rate: 0.0010\n",
      "Epoch 7/220\n",
      "\u001b[1m68/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0020\n",
      "Epoch 7: val_loss did not improve from 0.00312\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.0020 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 8/220\n",
      "\u001b[1m67/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019\n",
      "Epoch 8: val_loss improved from 0.00312 to 0.00287, saving model to E:/cryptics/dataset/best_model_tomato.keras\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0019 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 9/220\n",
      "\u001b[1m67/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0016\n",
      "Epoch 9: val_loss improved from 0.00287 to 0.00286, saving model to E:/cryptics/dataset/best_model_tomato.keras\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - loss: 0.0016 - val_loss: 0.0029 - learning_rate: 0.0010\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Train data RMSE:  7.376431625449675\n",
      "Train data MSE:  54.41174352493414\n",
      "Train data MAE:  5.3314884273284076\n",
      "-------------------------------------------------------------------------------------\n",
      "Test data RMSE:  9.541151728336905\n",
      "Test data MSE:  91.03357630314629\n",
      "Test data MAE:  6.647113526072862\n",
      "Train data explained variance regression score: 0.8356615560513531\n",
      "Test data explained variance regression score: 0.8398586585221881\n",
      "Train data R2 score: 0.83076224291924\n",
      "Test data R2 score: 0.8397046147681584\n",
      "Train data MGD:  0.020890523213438355\n",
      "Test data MGD:  0.018585732178988842\n",
      "----------------------------------------------------------------------\n",
      "Train data MPD:  1.0022034854588993\n",
      "Test data MPD:  1.2192072314993787\n",
      "Train predicted data:  (3159, 1)\n",
      "Test predicted data:  (3159, 1)\n",
      "Output of predicted next days:  30\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30]\n",
      "[31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59 60]\n",
      "[65.00000000000001, 65.00000000000001, 65.00000000000001, 65.00000000000001, 65.00000000000001, 75.00000000000001, 65.00000000000001, 65.00000000000001, 65.00000000000001, 65.00000000000001, 65.00000000000001, 55.00000000000001, 65.00000000000001, 65.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001, 55.00000000000001]\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('E:/cryptics/dataset/Tomato_Big.csv')\n",
    "df\n",
    "closedf = df[['sn','price']]\n",
    "print(\"Shape of close dataframe:\", closedf.shape)\n",
    "del closedf['sn']\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "closedf=scaler.fit_transform(np.array(closedf).reshape(-1,1))\n",
    "print(closedf.shape)\n",
    "\n",
    "training_size=int(len(closedf)*0.70)\n",
    "test_size=len(closedf)-training_size\n",
    "train_data,test_data=closedf[0:training_size,:],closedf[training_size:len(closedf),:1]\n",
    "\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "time_step = 30\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "\n",
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "\n",
    "#red potato\n",
    "model=Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=(None,1),activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "\n",
    "#early stopping and model checking\n",
    "\n",
    "#Early Stopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', #value being monitored for improvement\n",
    "                          min_delta = 0.001,         #Abs value and is the main change required before we stop\n",
    "                          mode='auto',\n",
    "                          patience = 6,             #no of epocs we wait before stopping\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keep the best weigts once stopped\n",
    "#Model Checkpoint\n",
    "checkpoint = ModelCheckpoint(monitor = \"val_loss\",\n",
    "                             mode ='auto',\n",
    "                             filepath = \"E:/cryptics/dataset/best_model_tomato.keras\",\n",
    "                             verbose = 1,\n",
    "                             save_best_only = True,\n",
    "                             )\n",
    "\n",
    "#Reduce learning Rate\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.01,\n",
    "                              patience = 6,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.001)\n",
    "\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callback = [earlystop, checkpoint,reduce_lr]\n",
    "\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=callback,epochs=220,batch_size=32,verbose=1)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "\n",
    "\n",
    "### Lets Do the prediction and check performance metrics\n",
    "train_predict=model.predict(X_train)\n",
    "test_predict=model.predict(X_test)\n",
    "train_predict.shape, test_predict.shape\n",
    "\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "original_ytrain = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "original_ytest = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "\n",
    "# Evaluation metrices RMSE and MAE\n",
    "print(\"Train data RMSE: \", math.sqrt(mean_squared_error(original_ytrain,train_predict)))\n",
    "print(\"Train data MSE: \", mean_squared_error(original_ytrain,train_predict))\n",
    "print(\"Train data MAE: \", mean_absolute_error(original_ytrain,train_predict))\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"Test data RMSE: \", math.sqrt(mean_squared_error(original_ytest,test_predict)))\n",
    "print(\"Test data MSE: \", mean_squared_error(original_ytest,test_predict))\n",
    "print(\"Test data MAE: \", mean_absolute_error(original_ytest,test_predict))\n",
    "\n",
    "print(\"Train data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytrain, train_predict))\n",
    "print(\"Test data explained variance regression score:\",\n",
    "      explained_variance_score(original_ytest, test_predict))\n",
    "\n",
    "\n",
    "print(\"Train data R2 score:\", r2_score(original_ytrain, train_predict))\n",
    "print(\"Test data R2 score:\", r2_score(original_ytest, test_predict))\n",
    "\n",
    "print(\"Train data MGD: \", mean_gamma_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MGD: \", mean_gamma_deviance(original_ytest, test_predict))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "print(\"Train data MPD: \", mean_poisson_deviance(original_ytrain, train_predict))\n",
    "print(\"Test data MPD: \", mean_poisson_deviance(original_ytest, test_predict))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "look_back=time_step\n",
    "trainPredictPlot = np.empty_like(closedf)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "print(\"Train predicted data: \", trainPredictPlot.shape)\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(closedf)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict)+(look_back*2)+1:len(closedf)-1, :] = test_predict\n",
    "print(\"Test predicted data: \", testPredictPlot.shape)\n",
    "\n",
    "\n",
    "x_input=test_data[len(test_data)-time_step:].reshape(1,-1)\n",
    "temp_input=list(x_input)\n",
    "temp_input=temp_input[0].tolist()\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "lst_output=[]\n",
    "n_steps=time_step\n",
    "i=0\n",
    "pred_days = 30    # Prediction for next 30 days\n",
    "\n",
    "while(i<pred_days):\n",
    "\n",
    "    if(len(temp_input)>time_step):\n",
    "\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        #print(\"{} day input {}\".format(i,x_input))\n",
    "        x_input = x_input.reshape(1,-1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        #print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "    else:\n",
    "\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i=i+1\n",
    "\n",
    "print(\"Output of predicted next days: \", len(lst_output))\n",
    "\n",
    "last_days=np.arange(1,time_step+1)\n",
    "day_pred=np.arange(time_step+1,time_step+pred_days+1)\n",
    "print(last_days)\n",
    "print(day_pred)\n",
    "\n",
    "temp_mat = np.empty((len(last_days)+pred_days+1,1))\n",
    "temp_mat[:] = np.nan\n",
    "temp_mat = temp_mat.reshape(1,-1).tolist()[0]\n",
    "\n",
    "last_original_days_value = temp_mat\n",
    "next_predicted_days_value = temp_mat\n",
    "\n",
    "last_original_days_value[0:time_step+1] = scaler.inverse_transform(closedf[len(closedf)-time_step:]).reshape(1,-1).tolist()[0]\n",
    "next_predicted_days_value[time_step+1:] = scaler.inverse_transform(np.array(lst_output).reshape(-1,1)).reshape(1,-1).tolist()[0]\n",
    "\n",
    "new_pred_plot = pd.DataFrame({\n",
    "    'last_original_days_value':last_original_days_value,\n",
    "    'next_predicted_days_value':next_predicted_days_value\n",
    "})\n",
    "\n",
    "names = cycle(['Last 15 days close price','Predicted next 30 days close price'])\n",
    "\n",
    "\n",
    "# Assuming you have new_pred_plot DataFrame with necessary columns\n",
    "\n",
    "\n",
    "\n",
    "lstmdf=closedf.tolist()\n",
    "lstmdf.extend((np.array(lst_output).reshape(-1,1)).tolist())\n",
    "lstmdf=scaler.inverse_transform(lstmdf).reshape(1,-1).tolist()[0]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have lstmdf list of stock prices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(next_predicted_days_value[:30])\n",
    "print(len(next_predicted_days_value))\n",
    "current3 =df.iloc[len(df)-1]\n",
    "yesterday3 =df.iloc[len(df)-2]\n",
    "\n",
    "predictedprices3=next_predicted_days_value[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377aa824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.0, 28.999999999999996, 30.0, 30.0, 28.999999999999996, 28.999999999999996, 28.999999999999996, 29.25, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 28.999999999999996, 31.0, 31.0, 32.5, 32.0, 32.0, 34.0, 34.0, 33.8, 33.33, 34.0, 33.6, 33.4, 33.5, 33.0, 33.5]\n",
      "[130.0, 130.0, 130.0, 130.0, 130.0, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 133.33, 130.0, 130.0, 130.0, 130.0, 130.0, 140.0, 145.0, 136.67, 135.0, 136.67, 125.00000000000001, 125.00000000000001, 130.0, 130.0, 130.0, 130.0]\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:27333\n",
      "Press CTRL+C to quit\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_7856\\3134589406.py:16: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_7856\\3134589406.py:17: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_7856\\3134589406.py:25: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_7856\\3134589406.py:26: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_7856\\3134589406.py:33: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_7856\\3134589406.py:34: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_7856\\3134589406.py:41: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "C:\\Users\\itssh\\AppData\\Local\\Temp\\ipykernel_7856\\3134589406.py:42: FutureWarning:\n",
      "\n",
      "Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "\n",
      "127.0.0.1 - - [10/Mar/2024 23:02:49] \"GET /getPotatoValues HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "print(predictedprices)\n",
    "print(predictedprices1)\n",
    "\n",
    "@app.route('/getPotatoValues', methods=['GET'])\n",
    "def get_potato_values():\n",
    "    # Ensure there are at least 3 values in the predicted_prices array\n",
    "    if len(predictedprices) >= 3:\n",
    "        potato_data = {\n",
    "            \"name\": \"potato\",\n",
    "            \"currentPrice\": math.ceil(current[-1]),\n",
    "            \"yesterdayPrice\":math.ceil( yesterday[-1]),\n",
    "            \"tomorrowPrice\": math.ceil(predictedprices[0]),\n",
    "            \"description\": \"This is potato\",\n",
    "            \"locality\": \"Nepali\",\n",
    "        }\n",
    "        \n",
    "        banana_data = {\n",
    "            \"name\": \"banana\",\n",
    "            \"currentPrice\": math.ceil(current1[-1]),\n",
    "            \"yesterdayPrice\": math.ceil(yesterday1[-1]),\n",
    "            \"tomorrowPrice\": math.ceil(predictedprices1[0]),\n",
    "            \"description\": \"This is Banana\",\n",
    "            \"locality\": \"Indian\",\n",
    "        }\n",
    "        cauli_data = {\n",
    "            \"name\": \"Cauli\",\n",
    "            \"currentPrice\": math.ceil(current2[-1]),\n",
    "            \"yesterdayPrice\":math.ceil( yesterday2[-1]),\n",
    "            \"tomorrowPrice\": math.ceil(predictedprices2[0]),\n",
    "            \"description\": \"This is Local Cauli\",\n",
    "            \"locality\": \"Nepali\",\n",
    "        }\n",
    "        tomato_data = {\n",
    "            \"name\": \"Big Tomato\",\n",
    "            \"currentPrice\":math.ceil( current3[-1]),\n",
    "            \"yesterdayPrice\":math.ceil( yesterday3[-1]),\n",
    "            \"tomorrowPrice\": math.ceil(predictedprices3[0]),\n",
    "            \"description\": \"This is Local Hybrid Tomato\",\n",
    "            \"locality\": \"Nepali\",\n",
    "        }\n",
    "        return jsonify(potato_data,banana_data,cauli_data,tomato_data)\n",
    "    else:\n",
    "        return jsonify({\"error\": \"Not enough data available\"}), 404\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(port=27333)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
